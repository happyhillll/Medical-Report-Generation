{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modeling_medical_report_generation.ipynb",
      "provenance": [],
      "mount_file_id": "1jf7kTs3laFjEqPm8DdNo3E7x88IQPXIT",
      "authorship_tag": "ABX9TyMCNicAD+0r6Wz7SF4ZQhyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/happyhillll/Medical-Report-Generation/blob/main/modeling_medical_report_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import"
      ],
      "metadata": {
        "id": "Tw_jJvSHj2hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import sklearn\n",
        "import tqdm\n",
        "from tqdm import tqdm \n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Input,Dense,Conv2D,concatenate,Dropout,LSTM\n",
        "from keras import Model\n",
        "from tensorflow.keras import activations\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk.translate.bleu_score as bleu\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "M2iz2uuXj5gI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "733kuNJKkQrV",
        "outputId": "3a85f35b-1483-4b2e-d51d-bbeb92c45823"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/하울스 1팀 : medical report generation/Self Case Study 2\")  "
      ],
      "metadata": {
        "id": "AG2t3kSckW9r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=pd.read_csv(\"data.csv\") "
      ],
      "metadata": {
        "id": "T3Yqt_yDk-1V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the feature extraction model"
      ],
      "metadata": {
        "id": "y6_LMHYClImg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_chexnet_model=load_model(\"chexnet.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "krDdaDeqlKlc",
        "outputId": "3a35a7ba-7bd6-416e-ed73-a875fbd56dbd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-98a0fbaa1f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_chexnet_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chexnet.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             raise ImportError(\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at chexnet.h5"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction(image_1,image_2):\n",
        "\n",
        "  #normalize the values of the image\n",
        "  image_1=image_1/255\n",
        "  image_2=image_2/255\n",
        "\n",
        "    #resize all image into (224,224)\n",
        "  image_1 = cv2.resize(image_1,(224,224))\n",
        "  image_2 = cv2.resize(image_2,(224,224))\n",
        "    \n",
        "  image_1= np.expand_dims(image_1, axis=0)\n",
        "  image_2= np.expand_dims(image_2, axis=0)\n",
        "    \n",
        "    #now we have read two image per patient. this is goven to the chexnet model for feature extraction\n",
        "    \n",
        "  image_1_out=final_chexnet_model(image_1)\n",
        "  image_2_out=final_chexnet_model(image_2)\n",
        "  #conactenate along the width\n",
        "  conc=np.concatenate((image_1_out,image_2_out),axis=2)\n",
        "  #reshape into(no.of images passed, length*breadth, depth)\n",
        "  image_feature=tf.reshape(conc, (conc.shape[0], -1, conc.shape[-1]))\n",
        "  \n",
        "\n",
        "  \n",
        "  return image_feature"
      ],
      "metadata": {
        "id": "KAS9Yg3rlTyL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "a=random.sample(range(764),1)[0]\n",
        "\n",
        "print(a)\n",
        "img1=test.iloc[a][\"image1\"] \n",
        "img2=test.iloc[a][\"image2\"]\n",
        "image_1 = Image.open(img1)\n",
        "  \n",
        "image_1= np.asarray(image_1.convert(\"RGB\"))\n",
        "  \n",
        "  \n",
        "image_2 = Image.open(img2)\n",
        "image_2 = np.asarray(image_2.convert(\"RGB\"))\n",
        "  \n",
        "image_feature=feature_extraction(image_1,image_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HYjItaWglWQv",
        "outputId": "bbfe837e-1a8d-43ee-ef1b-087d0aee63fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6f31aefc7fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mimage_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimage_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-e9f1d009c332>\u001b[0m in \u001b[0;36mfeature_extraction\u001b[0;34m(image_1, image_2)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#now we have read two image per patient. this is goven to the chexnet model for feature extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mimage_1_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_chexnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mimage_2_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_chexnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m#conactenate along the width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_chexnet_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix=np.load('emb.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "7BPEa5XHlYz4",
        "outputId": "4eb1b485-52d0-483b-bd50-b25441ee7742"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7c25d0ddac4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emb.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'emb.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder model\n",
        "'''\n",
        "here the input will be image features with size (96,1024). We can consider this tensor as the encoder output.\n",
        "But here we add another dense layer that will reduce the depth of this feature from 1024 to a low value\n",
        "'''\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,units):\n",
        "    super().__init__()\n",
        "    self.units=units\n",
        "    \n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.dense1=Dense(self.units,activation=\"relu\",name=\"encoder_dense\")\n",
        "    self.maxpool=tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "  def call(self,input_):\n",
        "    enc_out=self.maxpool(input_)\n",
        "    enc_out=self.dense1(enc_out) \n",
        "    \n",
        "    return enc_out\n",
        "    \n",
        "  def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state\n",
        "      If batch size is 32- Hidden state shape is [32,units]\n",
        "      '''\n",
        "      forward_h=tf.zeros((batch_size,self.units))\n",
        "      back_h=tf.zeros((batch_size,self.units))\n",
        "      return forward_h,back_h"
      ],
      "metadata": {
        "id": "zC1QmVQnlapA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "this is the attention class. \n",
        "Here the input to the decoder and the gru hidden state at the pevious time step are given, and the context vector is calculated\n",
        "\n",
        "This context vector is calculated uisng the attention weights. This context vector is then passed to the decoder model\n",
        "\n",
        "Here conact function is used for calaculating the attention weights\n",
        "\n",
        "'''\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,att_units):\n",
        "\n",
        "    super().__init__()\n",
        "    \n",
        "    self.att_units=att_units\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.wa=tf.keras.layers.Dense(self.att_units)\n",
        "    self.wb=tf.keras.layers.Dense(self.att_units)\n",
        "    self.v=tf.keras.layers.Dense(1)\n",
        "  \n",
        "    \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "   \n",
        "    x=tf.expand_dims(decoder_hidden_state,1)\n",
        "    \n",
        "    # print(x.shape)\n",
        "    # print(encoder_output.shape)\n",
        "      \n",
        "    alpha_dash=self.v(tf.nn.tanh(self.wa(encoder_output)+self.wb(x)))\n",
        "    \n",
        "    alphas=tf.nn.softmax(alpha_dash,1)\n",
        "\n",
        "    # print(\"en\",encoder_output.shape)\n",
        "    # print(\"al\",alphas.shape)\n",
        "    \n",
        "    context_vector=tf.matmul(encoder_output,alphas,transpose_a=True)[:,:,0]\n",
        "    # context_vector = alphas*encoder_output\n",
        "    # print(\"c\",context_vector.shape)\n",
        "\n",
        "\n",
        "    return (context_vector,alphas)\n",
        "        "
      ],
      "metadata": {
        "id": "6FB99yaFlePb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This class will perform the decoder task.\n",
        "The main decoder will call this onestep decoder at every time step. This one step decoder in turn class the atention model and return the ouptput \n",
        "at time step t.\n",
        "This output is passed through the final softmax layer with output size =vocab size, and pass this result to the main decoder model\n",
        "\n",
        "'''\n",
        "\n",
        "class One_Step_Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size, embedding_dim, input_length, dec_units ,att_units):\n",
        "\n",
        "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "    super().__init__()\n",
        "    \n",
        "    self.att_units=att_units\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.input_length=input_length\n",
        "    \n",
        "    self.dec_units=dec_units\n",
        "    self.attention=Attention(self.att_units)\n",
        "  #def build(self,inp_shape):\n",
        "    self.embedding=tf.keras.layers.Embedding(self.vocab_size,output_dim=self.embedding_dim,\n",
        "                                             input_length=self.input_length,mask_zero=True,trainable=False,weights=[embedding_matrix])\n",
        "\n",
        "    self.gru= tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.dec_units,return_sequences=True, return_state=True))\n",
        "    self.dense=tf.keras.layers.Dense(self.vocab_size,name=\"decoder_final_dense\") \n",
        "    self.dense_2=tf.keras.layers.Dense(self.embedding_dim,name=\"decoder_dense2\")\n",
        "\n",
        "  def call(self,input_to_decoder, encoder_output, for_h,bac_h):\n",
        "    \n",
        "    embed=self.embedding(input_to_decoder)\n",
        "    state_h=tf.keras.layers.Add()([for_h,bac_h])\n",
        "    \n",
        "\n",
        "    context_vector,alpha=self.attention(state_h,encoder_output)\n",
        "    context_vector=self.dense_2(context_vector)\n",
        "    \n",
        "    result=tf.concat([tf.expand_dims(context_vector, axis=1),embed],axis=-1)\n",
        "    \n",
        "   \n",
        "    output,forward_h,back_h=self.gru(result,initial_state=[for_h,bac_h])\n",
        "    out=tf.reshape(output,(-1,output.shape[-1]))\n",
        "\n",
        "    out=tf.keras.layers.Dropout(0.5)(out)\n",
        "    \n",
        "    dense_op=self.dense(out)\n",
        "    \n",
        "    return dense_op,forward_h,back_h,alpha"
      ],
      "metadata": {
        "id": "-hlPFPEblgGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For every input sentence, each output word is generated using one step decoder. Each output word is stored using the final decoder model and the\n",
        "final output sentence is returned\n",
        "\n",
        "'''\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_length, dec_units,att_units):\n",
        "      super().__init__()\n",
        "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "      self.onestep=One_Step_Decoder(vocab_size, embedding_dim, output_length, dec_units,att_units)\n",
        "\n",
        "\n",
        "        \n",
        "    def call(self, input_to_decoder,encoder_output,state_1,state_2):\n",
        "\n",
        "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "        #Create a tensor array as shown in the reference notebook\n",
        "        \n",
        "        #Iterate till the length of the decoder input\n",
        "            # Call onestepdecoder for each token in decoder_input\n",
        "            # Store the output in tensorarray\n",
        "        # Return the tensor array\n",
        "        \n",
        "        all_outputs=tf.TensorArray(tf.float32,input_to_decoder.shape[1],name=\"output_array\")\n",
        "        for step in range(input_to_decoder.shape[1]):\n",
        "          output,state_1,state_2,alpha=self.onestep(input_to_decoder[:,step:step+1],encoder_output,state_1,state_2)\n",
        "\n",
        "          all_outputs=all_outputs.write(step,output)\n",
        "        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n",
        "        \n",
        "        return all_outputs\n"
      ],
      "metadata": {
        "id": "jxvE8CP5liN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "  def __init__(self,enc_units,embedding_dim,vocab_size,output_length,dec_units,att_units,batch_size):\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "        self.batch_size=batch_size\n",
        "        self.encoder =Encoder(enc_units)\n",
        "        self.decoder=Decoder(vocab_size,embedding_dim,output_length,dec_units,att_units)\n",
        "        \n",
        "  \n",
        "    #Coompute the image features using feature extraction model and pass it to the encoder\n",
        "    # This will give encoder ouput\n",
        "   # Pass the decoder sequence,encoder_output,initial states to Decoder\n",
        "    # return the decoder output\n",
        "\n",
        "  \n",
        "  def call(self, data):\n",
        "        features,report  = data[0], data[1]\n",
        "        \n",
        "        encoder_output= self.encoder(features)\n",
        "        state_h,back_h=self.encoder.initialize_states(self.batch_size)\n",
        "        \n",
        "        output= self.decoder(report, encoder_output,state_h,back_h)\n",
        "      \n",
        "        return output\n"
      ],
      "metadata": {
        "id": "nd7JGMBdlkq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_units=64\n",
        "embedding_dim=300\n",
        "dec_units=64\n",
        "att_units=64\n",
        "max_len=80\n",
        "vocab_size=2017\n",
        "bs=5\n",
        "model  = encoder_decoder(enc_units,embedding_dim,vocab_size,max_len,dec_units,att_units,bs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "AyMrZTOWlm-y",
        "outputId": "54ed61a2-ad94-4e86-d322-9a982191f8be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a0ba7ac4bd44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matt_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder_decoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"model2wts/bidir_fit_15_b\") "
      ],
      "metadata": {
        "id": "Sj6Iw4gGlnkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the pretrained tokenizer\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    token= pickle.load(handle)"
      ],
      "metadata": {
        "id": "vT6I3f0blo7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_second(elem):\n",
        "    return elem[1]\n",
        "\n",
        "def beam_search(image_features, beam_index):\n",
        "\n",
        "    hidden_state =  tf.zeros((1, enc_units))\n",
        "\n",
        "    hidden_state_2 =  tf.zeros((1, enc_units))\n",
        "    encoder_out = model.layers[0](image_features)\n",
        "\n",
        "    start_token = [token.word_index[\"<sos>\"]]\n",
        "    dec_word = [[start_token, 0.0]]\n",
        "    while len(dec_word[0][0]) < max_len:\n",
        "        temp = []\n",
        "        for word in dec_word:\n",
        "            \n",
        "            predict, hidden_state,hidden_state_2,alpha = model.layers[1].onestep(tf.expand_dims([word[0][-1]],1), encoder_out, hidden_state,hidden_state_2)\n",
        "           \n",
        "           \n",
        "            word_predict = np.argsort(predict[0])[-beam_index:]\n",
        "            for i in word_predict:\n",
        "\n",
        "                next_word, probab = word[0][:], word[1]\n",
        "                next_word.append(i)\n",
        "                probab += predict[0][i] \n",
        "                temp.append([next_word, probab.numpy()])\n",
        "        dec_word = temp\n",
        "        # Sorting according to the probabilities scores\n",
        "        \n",
        "        \n",
        "        dec_word = sorted(dec_word, key=take_second)\n",
        "       \n",
        "        # Getting the top words\n",
        "        dec_word = dec_word[-beam_index:] \n",
        "        \n",
        "     \n",
        "    final = dec_word[-1]\n",
        "    \n",
        "    report =final[0]\n",
        "    score = final[1]\n",
        "    temp = []\n",
        "    \n",
        "    for word in report:\n",
        "      if word!=0:\n",
        "        if word != token.word_index['<eos>']:\n",
        "            temp.append(token.index_word[word])\n",
        "        else:\n",
        "            break \n",
        "\n",
        "    rep = ' '.join(e for e in temp)        \n",
        "    \n",
        "    return rep, score"
      ],
      "metadata": {
        "id": "_Do6-5YWlqKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result,score=beam_search(image_feature,3) \n",
        "\n",
        "actual=test[\"report\"][a]\n",
        "  \n",
        "print(\"ACTUAL REPORT: \",actual)\n",
        "print(\"GENERATED REPORT: \",result)\n",
        "\n",
        "print(\"BLEU SCORE IS: \",bleu.sentence_bleu(actual,result))   "
      ],
      "metadata": {
        "id": "gnRBc_Rclsac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}